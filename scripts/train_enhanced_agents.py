#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆå¤šæ™ºèƒ½ä½“åŒ»ç–—ç³»ç»Ÿå¼ºåŒ–å­¦ä¹ è®­ç»ƒè„šæœ¬

åŸºäºçœŸå®MIMIC-IIIæ•°æ®è®­ç»ƒPPOæ™ºèƒ½ä½“
"""

import sys
import os
import numpy as np
import pandas as pd
from pathlib import Path
import time
import json
from typing import Dict, List, Any
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¼ºåŒ–å­¦ä¹ ç›¸å…³å¯¼å…¥
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback
from stable_baselines3.common.logger import configure
import torch

# é¡¹ç›®ç»„ä»¶å¯¼å…¥
from env.enhanced_multi_agent_env import EnhancedMultiAgentHealthcareEnv
from pettingzoo.utils.conversions import parallel_wrapper_fn

class TrainingCallback(BaseCallback):
    """è®­ç»ƒè¿‡ç¨‹ç›‘æ§å›è°ƒ"""
    
    def __init__(self, save_freq: int = 10000, verbose: int = 1):
        super().__init__(verbose)
        self.save_freq = save_freq
        self.training_logs = []
    
    def _on_step(self) -> bool:
        # æ¯save_freqæ­¥è®°å½•ä¸€æ¬¡
        if self.n_calls % self.save_freq == 0:
            info = {
                'step': self.n_calls,
                'total_timesteps': self.locals.get('total_timesteps', 0),
                'fps': getattr(self.model, '_last_log_fps', 0),
                'episode_reward_mean': getattr(self.model, '_last_ep_reward_mean', 0),
                'episode_length_mean': getattr(self.model, '_last_ep_length_mean', 0),
            }
            self.training_logs.append(info)
            
            if self.verbose > 0:
                print(f"æ­¥éª¤ {self.n_calls:,}: å¹³å‡å¥–åŠ± {info['episode_reward_mean']:.3f}, "
                      f"å¹³å‡é•¿åº¦ {info['episode_length_mean']:.1f}, FPS {info['fps']:.1f}")
        
        return True
    
    def save_logs(self, filepath: str):
        """ä¿å­˜è®­ç»ƒæ—¥å¿—"""
        with open(filepath, 'w') as f:
            json.dump(self.training_logs, f, indent=2)

def create_single_agent_env():
    """åˆ›å»ºå•æ™ºèƒ½ä½“åŒ…è£…çš„ç¯å¢ƒ"""
    # åˆ›å»ºå¢å¼ºç‰ˆç¯å¢ƒ
    base_env = EnhancedMultiAgentHealthcareEnv(
        render_mode="rgb_array", 
        use_real_data=True
    )
    
    # ä¸ºè®­ç»ƒåŒ…è£…æˆå•æ™ºèƒ½ä½“ç¯å¢ƒ - åªè®­ç»ƒåŒ»ç”Ÿæ™ºèƒ½ä½“
    class SingleAgentWrapper(gym.Env):
        def __init__(self, multi_agent_env):
            super().__init__()
            self.env = multi_agent_env
            self.observation_space = multi_agent_env.observation_space("doctor")
            self.action_space = multi_agent_env.action_space("doctor")
            self.current_agent = "doctor"
            
        def reset(self, **kwargs):
            observations, infos = self.env.reset(**kwargs)
            doctor_obs = observations.get("doctor", np.zeros(self.observation_space.shape))
            doctor_info = infos.get("doctor", {})
            return doctor_obs, doctor_info
        
        def step(self, action):
            # ä¸ºåŒ»ç”Ÿæ™ºèƒ½ä½“æ‰§è¡ŒåŠ¨ä½œ
            if self.env.agent_selection == "doctor":
                self.env.step(action)
            
            # ä¸ºå…¶ä»–æ™ºèƒ½ä½“ç”Ÿæˆç®€å•åŠ¨ä½œ
            while self.env.agent_selection != "doctor" and not all(self.env.terminations.values()):
                if self.env.agent_selection == "patient":
                    # æ‚£è€…é…åˆæ²»ç–—
                    patient_action = [5, 6, 3, 5]  # ä¸­ç­‰é…åˆåº¦
                elif self.env.agent_selection == "insurance":
                    # ä¿é™©é€‚åº¦æ‰¹å‡†
                    insurance_action = [3, 2, 1, 5]  # æ ‡å‡†å®¡æ‰¹
                else:
                    patient_action = [0, 0, 0, 0]
                    insurance_action = [0, 0, 0, 0]
                
                self.env.step(patient_action if self.env.agent_selection == "patient" else insurance_action)
            
            # è·å–åŒ»ç”Ÿçš„è§‚å¯Ÿå’Œå¥–åŠ±
            obs = self.env.observations.get("doctor", np.zeros(self.observation_space.shape))
            reward = self.env.rewards.get("doctor", 0)
            terminated = self.env.terminations.get("doctor", False)
            truncated = self.env.truncations.get("doctor", False)
            info = {}
            
            return obs, reward, terminated, truncated, info
        
        def close(self):
            self.env.close()
    
    return SingleAgentWrapper(base_env)

def train_single_agent(config: Dict[str, Any]):
    """è®­ç»ƒå•ä¸ªPPOæ™ºèƒ½ä½“ï¼ˆåŒ»ç”Ÿï¼‰"""
    print("ğŸ¥ å¼€å§‹è®­ç»ƒåŒ»ç”Ÿæ™ºèƒ½ä½“ï¼ˆPPOç®—æ³•ï¼‰")
    
    # åˆ›å»ºè®­ç»ƒç›®å½•
    models_dir = Path("models")
    logs_dir = Path("logs") 
    models_dir.mkdir(exist_ok=True)
    logs_dir.mkdir(exist_ok=True)
    
    # åˆ›å»ºç¯å¢ƒ
    print("ğŸ“‹ åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    env = create_single_agent_env()
    
    # é…ç½®æ—¥å¿—
    try:
        logger = configure(str(logs_dir / "ppo_doctor"), ["stdout", "csv", "tensorboard"])
    except Exception:
        logger = configure(str(logs_dir / "ppo_doctor"), ["stdout", "csv"])
    
    # åˆ›å»ºPPOæ¨¡å‹
    print("ğŸ§  åˆå§‹åŒ–PPOæ¨¡å‹...")
    model = PPO(
        policy="MlpPolicy",
        env=env,
        learning_rate=config.get("learning_rate", 3e-4),
        n_steps=config.get("n_steps", 2048),
        batch_size=config.get("batch_size", 64),
        n_epochs=config.get("n_epochs", 10),
        gamma=config.get("gamma", 0.99),
        gae_lambda=config.get("gae_lambda", 0.95),
        clip_range=config.get("clip_range", 0.2),
        ent_coef=config.get("ent_coef", 0.01),
        vf_coef=config.get("vf_coef", 0.5),
        max_grad_norm=config.get("max_grad_norm", 0.5),
        verbose=1,
        device=config.get("device", "auto")
    )
    
    # è®¾ç½®æ—¥å¿—è®°å½•å™¨
    model.set_logger(logger)
    
    # åˆ›å»ºå›è°ƒ
    training_callback = TrainingCallback(save_freq=config.get("log_freq", 10000))
    checkpoint_callback = CheckpointCallback(
        save_freq=config.get("save_freq", 50000),
        save_path=str(models_dir),
        name_prefix="ppo_doctor"
    )
    
    # å¼€å§‹è®­ç»ƒ
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {config['total_timesteps']:,} æ­¥...")
    start_time = time.time()
    
    model.learn(
        total_timesteps=config["total_timesteps"],
        callback=[training_callback, checkpoint_callback],
        progress_bar=True
    )
    
    training_time = time.time() - start_time
    print(f"âœ… è®­ç»ƒå®Œæˆï¼è€—æ—¶: {training_time:.1f}ç§’")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = models_dir / "ppo_doctor_final"
    model.save(str(final_model_path))
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")
    
    # ä¿å­˜è®­ç»ƒæ—¥å¿—
    training_callback.save_logs(str(logs_dir / "training_logs.json"))
    
    # æ¸…ç†
    env.close()
    
    return model, training_callback.training_logs

def evaluate_trained_agent(model_path: str, n_episodes: int = 10):
    """è¯„ä¼°è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“"""
    print(f"\nğŸ“Š è¯„ä¼°è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“ ({n_episodes} episodes)")
    
    # åŠ è½½æ¨¡å‹
    model = PPO.load(model_path)
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    env = create_single_agent_env()
    
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(n_episodes):
        obs, info = env.reset()
        total_reward = 0
        episode_length = 0
        done = False
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            total_reward += reward
            episode_length += 1
        
        episode_rewards.append(total_reward)
        episode_lengths.append(episode_length)
        
        print(f"  Episode {episode + 1}: å¥–åŠ± {total_reward:.3f}, é•¿åº¦ {episode_length}")
    
    env.close()
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "mean_reward": np.mean(episode_rewards),
        "std_reward": np.std(episode_rewards),
        "mean_length": np.mean(episode_lengths),
        "std_length": np.std(episode_lengths),
        "episodes": episode_rewards
    }
    
    print(f"\nğŸ“ˆ è¯„ä¼°ç»“æœ:")
    print(f"  å¹³å‡å¥–åŠ±: {stats['mean_reward']:.3f} Â± {stats['std_reward']:.3f}")
    print(f"  å¹³å‡é•¿åº¦: {stats['mean_length']:.1f} Â± {stats['std_length']:.1f}")
    
    return stats

def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆå¤šæ™ºèƒ½ä½“åŒ»ç–—ç³»ç»Ÿè®­ç»ƒ")
    print("=" * 60)
    
    # è®­ç»ƒé…ç½®
    config = {
        "total_timesteps": 100000,  # è®­ç»ƒæ­¥æ•°
        "learning_rate": 3e-4,      # å­¦ä¹ ç‡
        "n_steps": 2048,            # æ¯æ¬¡æ”¶é›†çš„æ­¥æ•°
        "batch_size": 64,           # æ‰¹æ¬¡å¤§å°
        "n_epochs": 10,             # æ¯æ¬¡æ›´æ–°çš„è½®æ•°
        "gamma": 0.99,              # æŠ˜æ‰£å› å­
        "gae_lambda": 0.95,         # GAEå‚æ•°
        "clip_range": 0.2,          # PPOè£å‰ªèŒƒå›´
        "ent_coef": 0.01,           # ç†µç³»æ•°
        "vf_coef": 0.5,             # ä»·å€¼å‡½æ•°ç³»æ•°
        "max_grad_norm": 0.5,       # æ¢¯åº¦è£å‰ª
        "log_freq": 5000,           # æ—¥å¿—é¢‘ç‡
        "save_freq": 25000,         # ä¿å­˜é¢‘ç‡
        "device": "cuda" if torch.cuda.is_available() else "cpu"
    }
    
    print(f"ğŸ–¥ï¸  è®¾å¤‡: {config['device']}")
    print(f"ğŸ“Š è®­ç»ƒé…ç½®: {config['total_timesteps']:,} æ­¥")
    
    try:
        # è®­ç»ƒæ™ºèƒ½ä½“
        model, training_logs = train_single_agent(config)
        
        # è¯„ä¼°æ™ºèƒ½ä½“
        model_path = "models/ppo_doctor_final"
        eval_stats = evaluate_trained_agent(model_path, n_episodes=5)
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        results_dir = Path("data/training_results")
        results_dir.mkdir(exist_ok=True)
        
        with open(results_dir / "evaluation_results.json", "w") as f:
            json.dump(eval_stats, f, indent=2)
        
        print(f"\nğŸ‰ è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {results_dir}")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise

if __name__ == "__main__":
    main() 